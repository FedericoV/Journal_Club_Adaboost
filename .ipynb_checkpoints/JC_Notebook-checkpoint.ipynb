{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import colors\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import IPython.core.display\n",
    "\n",
    "%matplotlib inline\n",
    "IPython.core.display.display(IPython.core.display.HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of boring stuff to make the figures look similar to those in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rc('grid', linestyle=\"-\", color='black')\n",
    "cmap = colors.ListedColormap(['lightskyblue', 'orange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Classifiers:\n",
    "\n",
    "$$ y = \\beta X + \\sigma $$\n",
    "\n",
    "We are looking to estimate $\\beta$ as well as we possibly can from a limited number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate a non-linear dataset with lots of complex features\n",
    "X, y = make_friedman1(n_samples=100, n_features=55, random_state=42)\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal picture of overfitting with linear models.  We do very well on the training set, but generalize poorly because we fit too well to the idiosyncrasies of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and fit a linear model \n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Print R^2 of linear model on held out test data\n",
    "print (linear_model.score(X_train, y_train))\n",
    "print (linear_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression penalizes model complexity with an $L_2$ norm on the $\\beta$ coefficients enforcing simpler models.  We trade off train performance for test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train, y_train)\n",
    "# Print R^2 of linear model on held out test data\n",
    "\n",
    "print (ridge_model.score(X_train, y_train))\n",
    "print (ridge_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now: let's look at what adaboost is doing instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = DecisionTreeRegressor(max_depth=8)\n",
    "# Print R^2 of linear model on held out test data\n",
    "\n",
    "for n_estimators in [2, 5, 10, 50, 100, 500]:\n",
    "    \n",
    "    base_reg = DecisionTreeRegressor(max_depth=8)\n",
    "    ada_model = AdaBoostRegressor(base_estimator=base_reg, n_estimators=n_estimators)\n",
    "    ada_model.fit(X_train, y_train)\n",
    "    \n",
    "    print ('\\n Number of Esimators: {}'.format(n_estimators))\n",
    "    print (ada_model.score(X_train, y_train))\n",
    "    print (ada_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating Classifiers:\n",
    "\n",
    "![](Figures/Figure2.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y=None, clf=None, plot_step = 0.025,\n",
    "                           ax=None, n_classes=2, plot_colors = \"br\",\n",
    "                           plot_data=True):\n",
    "    \"\"\"\n",
    "    Adapted from: http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html\n",
    "    \"\"\"\n",
    "    Z = None\n",
    "    \n",
    "    if y is None:\n",
    "        n_classes = 1\n",
    "        y = np.array([0]*len(X))\n",
    "\n",
    "    x_min = y_min = 0\n",
    "    x_max = y_max = 1\n",
    "     \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(111, aspect='equal')\n",
    "        \n",
    "    if clf is not None:\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "        \n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = (Z.reshape(xx.shape) > 0.5).astype('float32')\n",
    "        hb = ax.imshow(Z, extent=(x_min, x_max, y_min, y_max), cmap=cmap)  \n",
    "    \n",
    "    if plot_data:\n",
    "        for i, color in zip(range(n_classes), plot_colors):\n",
    "            idx = np.where(y == i)\n",
    "            ax.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,\n",
    "                       s=10.0)\n",
    "                    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # major ticks every 20, minor ticks every 5                                      \n",
    "    major_ticks = np.linspace(0, 1, 6)                                              \n",
    "    minor_ticks = np.linspace(0, 1, 21)                                               \n",
    "\n",
    "    ax.set_xticks(major_ticks)                                                       \n",
    "    ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "    ax.set_yticks(major_ticks)                                                       \n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    \n",
    "    ax.grid(which='minor')\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Figure 1:\n",
    "n_dims = 2\n",
    "n_samples = 400\n",
    "p = 0.8\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, n_dims))\n",
    "\n",
    "# randomly make 75% of them red.\n",
    "y = np.random.uniform(size=n_samples)\n",
    "y = (y > p).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Absolutely necessary to replicate results.  Without\n",
    "# setting max depth we don't get the observed behaviour.\n",
    "base_clf = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=6)\n",
    "rf_clf = RandomForestClassifier(n_estimators=6)\n",
    "nn_clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "all_clfs = [ada_clf, rf_clf, nn_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 6))\n",
    "rates = {}\n",
    "\n",
    "for ax_idx, clf in enumerate(all_clfs):\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, ax_idx+1)\n",
    "    Z = plot_decision_boundary(X, y, clf=clf, ax=ax, plot_data=True)\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    rates[clf_name] = Z.sum() / Z.size\n",
    "    ax.set_title(clf_name, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 1:\n",
    "n_dims = 2\n",
    "n_samples = 20\n",
    "p = 0.7\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, n_dims))\n",
    "\n",
    "# randomly make 75% of them red.\n",
    "y = np.random.uniform(size=n_samples)\n",
    "y = (y > p).astype('int')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "for ax_idx, tree_clf in enumerate(rf_clf.estimators_):\n",
    "    \n",
    "    ax = fig.add_subplot(3, 3, ax_idx+1)\n",
    "    _ = plot_decision_boundary(X, y, clf=tree_clf, plot_step=0.01, ax=ax)\n",
    "    \n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "_ = plot_decision_boundary(X, y, clf=rf_clf, plot_step=0.01, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_dims = 2\n",
    "n_samples = 1000\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, n_dims))\n",
    "\n",
    "# points are in the circle if the vector norm\n",
    "# is bigger than 0.4 - subtract 0.5 with broadcasting\n",
    "# to center it\n",
    "in_circle = np.linalg.norm(X-0.5, axis=1) > 0.4\n",
    "outside_circle = in_circle == 0\n",
    "\n",
    "y = np.random.uniform(size=(n_samples,))\n",
    "\n",
    "y[in_circle] = y[in_circle] > 0.9  # 10% inside circle\n",
    "y[outside_circle] = y[outside_circle] > 0.1  # 90% inside circle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=500)\n",
    "rf_clf = RandomForestClassifier(n_estimators=500)\n",
    "nn_clf = KNeighborsClassifier(n_neighbors=1)\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "all_clfs = [nn_clf, ada_clf, rf_clf, tree_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "rates = {}\n",
    "\n",
    "for ax_idx, clf in enumerate(all_clfs):\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, ax_idx+1)\n",
    "    Z = plot_decision_boundary(X, y, clf=clf, ax=ax, plot_data=True)\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    rates[clf_name] = Z.sum() / Z.size\n",
    "    ax.set_title(clf_name, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_dims = 10\n",
    "n_samples = 4000\n",
    "\n",
    "base_clf = DecisionTreeClassifier(max_depth=8)\n",
    "ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=n_estimators)\n",
    "\n",
    "X_train = np.random.uniform(size=(n_samples, n_dims))\n",
    "y_train = np.random.uniform(size=(n_samples,)) > 0.8\n",
    "\n",
    "n_test_samples = 10000\n",
    "X_test = np.random.uniform(size=(n_test_samples, n_dims))\n",
    "\n",
    "recalculate_figures = False\n",
    "\n",
    "if recalculate_figures:\n",
    "    scores = []\n",
    "    for n_estimators in range(1, 250, 2):\n",
    "        \n",
    "        ada_clf.fit(X_train, y_train)\n",
    "\n",
    "        pred = ada_clf.predict(X_test)\n",
    "        pos_fraction = (pred.sum() / n_test_samples)\n",
    "\n",
    "        scores.append(pos_fraction)\n",
    "    scores = np.array(scores)\n",
    "    np.save('Scores_10_Dims.npy', scores)\n",
    "    \n",
    "else:\n",
    "    scores = np.load('Scores_10_Dims.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(scores)\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('Fraction of Positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-statistical view of Adaboost\n",
    "\n",
    "![](Figures/Adaboost_Decomposition.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More simulations: use Adaboost to tackle a harder problem.\n",
    "\n",
    "![](Figures/Border_Interpolation.png) \n",
    "\n",
    "### We interpreted the formula as: \n",
    "\n",
    "$$ P(y=1\\mid x) = 0.2 + 0.6 \\times \\Big(\\big(\\sum_{j=1}^2 x_j \\big) > 1 \\Big)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dims = 5\n",
    "n_samples = 400\n",
    "\n",
    "X_train = np.random.uniform(size=(n_samples, n_dims))\n",
    "y_train = 0.2 + 0.6 * np.sum(X_train[:, :2], axis=1) > 1\n",
    "\n",
    "X_test = np.random.uniform(size=(n_samples, n_dims))\n",
    "y_test = 0.2 + 0.6 * np.sum(X_test[:, :2], axis=1) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Error at Bayes Boundary\n",
    "\n",
    "![](Figures/Adaboost_Errors.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if recalculate_figures:\n",
    "\n",
    "    scores_dict = {}\n",
    "    \n",
    "    df = pd.read_csv('phonomene_dataset.csv', header=None)\n",
    "    y = df[5].values\n",
    "    X = df[list(range(5))].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    for estimator_depth in tqdm.tqdm_notebook(range(1, 8)):\n",
    "        scores_dict[('train', estimator_depth)] = {}\n",
    "        scores_dict[('test', estimator_depth)] = {}\n",
    "\n",
    "        for n_estimators in tqdm.tqdm_notebook(range(1, 250, 2)):\n",
    "            base_clf = DecisionTreeClassifier(max_depth=estimator_depth)\n",
    "            ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=n_estimators)\n",
    "\n",
    "            ada_clf.fit(X_train, y_train)\n",
    "\n",
    "            scores_dict[('train', estimator_depth)][n_estimators] = ada_clf.score(X_train, y_train)\n",
    "            scores_dict[('test', estimator_depth)][n_estimators] = ada_clf.score(X_test, y_test)\n",
    "\n",
    "        scores_df = pd.DataFrame(scores_dict)\n",
    "        scores_df.columns.rename(['dataset', 'max_depth'], inplace=True)\n",
    "        scores_df.to_csv('phonomene_scores.csv')\n",
    "else:\n",
    "    scores_df = pd.read_csv('phonomene_scores.csv', header=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for estimator_depth in tqdm.tqdm_notebook(range(1, 8)):\n",
    "    i = str(estimator_depth)\n",
    "    ax = fig.add_subplot(2, 4, estimator_depth)\n",
    "    ax.plot(1-scores_df['test'][i], color='black', label='test error')\n",
    "    ax.plot(1-scores_df['train'][i], color='red', label='train error')\n",
    "    \n",
    "    ax.set_ylim(-0.01, 0.25)\n",
    "    ax.set_title('Tree Depth: {}'.format(estimator_depth), fontsize=14)\n",
    "    ax.set_ylabel('Error')\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow or Deep Trees?\n",
    "\n",
    "![](Figures/Tree_Error.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key take-aways:\n",
    "\n",
    "- The statistical view of AdaBoost advocated by Hastie has a hard time explaining why AdaBoost keeps working long after you completely overfit the training set\n",
    "- AdaBoost works better using deep trees as base learners, rather than shallow trees (again, contra Hastie)\n",
    "- At 'steady state' the extra learners from AdaBoost help 'smooth out' the decision surface: similar to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Connections to deep learning?\n",
    "\n",
    "![](Figures/Deep_Learning_And_Generalization.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](Figures/NN_training.png) \n",
    "\n",
    "Commonly used neural network architectures can reach 100% accuracy on completely random data - they have sufficient capacity to perfectly memorize the training set.  How do those networks actually generalize?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "01ce9dd284df44b88e457b5322936e88": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "3779c50be1134db6af295bfcfe72e620": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "41b2e855561b44dea6733002ec2b6982": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "4f1874caebf34a64af17c2d447a4e342": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "65b298c5546a42efa60de01e2a40a774": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "7abcdd4275cd4b1ea9c34742f2e98bce": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "bb88555f081f4fa48704ee71b281f016": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "ca3c3f5f91aa4e27b1ebf5abdc1b7820": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "ee4246d9ce644317a94bc37f9588447a": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
